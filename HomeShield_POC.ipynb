{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1506a523",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ›¡ï¸ HomeShield AI â€” POC (Jupyter Notebook)\n",
    "\n",
    "This notebook demonstrates an end-to-end **Proof of Concept** for the **AI Claims Copilot with Coverage Validation**:\n",
    "\n",
    "- Ingest synthetic **policy documents** into **Pinecone**\n",
    "- Ask **coverage questions** with citations (RAG)\n",
    "- Submit a **claim** in natural language â†’ extract fields â†’ validate coverage (RAG) â†’ return decision with citations\n",
    "\n",
    "> **Prereqs**: You must have Azure OpenAI deployments and a Pinecone API key.  \n",
    "> **Data**: Point to your local `policies_docs/policies` folder (300 TXT files) and `homeshield_sample_data/customers.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a fresh environment, uncomment these:\n",
    "# !pip install fastapi uvicorn openai==1.* pinecone-client pandas python-dotenv tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdedd77",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, re, uuid, glob\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- UPDATE THESE PATHS FOR YOUR MACHINE ----\n",
    "POLICY_DIR   = r\"C:\\Users\\kalva\\AI_Projects\\HomeShield_AI\\policies_docs\\policies\"\n",
    "CUSTOMERS_CSV= r\"C:\\Users\\kalva\\AI_Projects\\HomeShield_AI\\homeshield_sample_data\\customers.csv\"\n",
    "\n",
    "# ---- ENV VARS (set here or via your OS/.env) ----\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",      \"https://<your-endpoint>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",       \"<your-azure-openai-key>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\",   \"2024-06-01\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-small\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",      \"gpt-4o\")\n",
    "\n",
    "os.environ.setdefault(\"PINECONE_API_KEY\", \"<your-pinecone-key>\")\n",
    "os.environ.setdefault(\"PINECONE_INDEX\",   \"homeshield-policies\")\n",
    "os.environ.setdefault(\"PINECONE_REGION\",  \"us-east-1\")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"POLICY_DIR exists:\", Path(POLICY_DIR).exists())\n",
    "print(\"CUSTOMERS_CSV exists:\", Path(CUSTOMERS_CSV).exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dffd1",
   "metadata": {},
   "source": [
    "## 2) Initialize Clients (Azure OpenAI + Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import AzureOpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_API_KEY  = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\",\"2024-06-01\")\n",
    "EMBED_MODEL = os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\",\"text-embedding-3-small\")\n",
    "CHAT_MODEL  = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",\"gpt-4o\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "PC_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "PINECONE_INDEX = os.environ.get(\"PINECONE_INDEX\",\"homeshield-policies\")\n",
    "PINECONE_REGION = os.environ.get(\"PINECONE_REGION\",\"us-east-1\")\n",
    "\n",
    "pc = Pinecone(api_key=PC_API_KEY)\n",
    "DIM = 1536 if \"small\" in EMBED_MODEL else 3072\n",
    "\n",
    "# Create index if missing\n",
    "if PINECONE_INDEX not in [i[\"name\"] for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX,\n",
    "        dimension=DIM,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_REGION),\n",
    "    )\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "print(\"Index ready:\", PINECONE_INDEX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b24992",
   "metadata": {},
   "source": [
    "## 3) Helpers: Chunking, Embedding, Retrieval (MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_batch(texts: List[str]) -> List[List[float]]:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def sentence_chunks(text: str, max_words=180, overlap=40) -> List[str]:\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    out, cur, n = [], [], 0\n",
    "    for s in sents:\n",
    "        w = len(s.split())\n",
    "        if n + w > max_words and cur:\n",
    "            out.append(\" \".join(cur))\n",
    "            tail = \" \".join(cur).split()[-overlap:]\n",
    "            cur = [\" \".join(tail), s]\n",
    "            n = len(cur[0].split()) + len(s.split())\n",
    "        else:\n",
    "            cur.append(s); n += w\n",
    "    if cur: out.append(\" \".join(cur))\n",
    "    return [c.strip() for c in out if c.strip()]\n",
    "\n",
    "def mmr(query_vec, cand_vecs, k=8, lam=0.5):\n",
    "    import numpy as np\n",
    "    q = np.array(query_vec)\n",
    "    C = np.array(cand_vecs)\n",
    "    sim = C @ q / ((np.linalg.norm(C, axis=1) * np.linalg.norm(q)) + 1e-8)\n",
    "    chosen, cand = [], list(range(len(cand_vecs)))\n",
    "    while len(chosen) < min(k, len(cand)):\n",
    "        best, best_score = None, -1e9\n",
    "        for i in cand:\n",
    "            div = 0 if not chosen else max(\n",
    "                (C[i] @ C[j]) / ((np.linalg.norm(C[i]) * np.linalg.norm(C[j])) + 1e-8)\n",
    "                for j in chosen\n",
    "            )\n",
    "            score = lam * sim[i] - (1 - lam) * div\n",
    "            if score > best_score:\n",
    "                best, best_score = i, score\n",
    "        chosen.append(best); cand.remove(best)\n",
    "    return chosen\n",
    "\n",
    "def retrieve(query: str, filters: Dict, k=8):\n",
    "    qv = embed_batch([query])[0]\n",
    "    res = index.query(vector=qv, top_k=max(20, k*2), include_metadata=True, filter=filters, namespace=\"policies\")\n",
    "    matches = res.matches or []\n",
    "    if not matches: return []\n",
    "    vecs = [m.values for m in matches]\n",
    "    keep = mmr(qv, vecs, k=min(k, len(matches)))\n",
    "    picked = [matches[i] for i in keep]\n",
    "    return [{\n",
    "        \"content\": m.metadata.get(\"content\",\"\"),\n",
    "        \"source\":  m.metadata.get(\"source\",\"\"),\n",
    "        \"page\":    m.metadata.get(\"page\", None)\n",
    "    } for m in picked], qv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26b562",
   "metadata": {},
   "source": [
    "## 4) Ingest policies into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = sorted(glob.glob(str(Path(POLICY_DIR) / \"*.txt\")))\n",
    "assert files, f\"No TXT files found under {POLICY_DIR}\"\n",
    "\n",
    "def parse_meta(fname: str) -> Dict:\n",
    "    base = os.path.basename(fname).replace(\".txt\",\"\")\n",
    "    _, plan, state, year = base.split(\"_\")\n",
    "    return {\"plan\": plan, \"state\": state, \"effective_year\": int(year)}\n",
    "\n",
    "total = 0\n",
    "for fpath in tqdm(files, desc=\"Upserting policies\"):\n",
    "    meta = parse_meta(fpath)\n",
    "    text = Path(fpath).read_text(encoding=\"utf-8\")\n",
    "    chunks = sentence_chunks(text)\n",
    "    if not chunks: \n",
    "        continue\n",
    "    BATCH = 64\n",
    "    for i in range(0, len(chunks), BATCH):\n",
    "        batch = chunks[i:i+BATCH]\n",
    "        vecs = embed_batch(batch)\n",
    "        points = []\n",
    "        for j, (ch, v) in enumerate(zip(batch, vecs)):\n",
    "            points.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"values\": v,\n",
    "                \"metadata\": {\n",
    "                    \"plan\": meta[\"plan\"],\n",
    "                    \"state\": meta[\"state\"],\n",
    "                    \"effective_year\": meta[\"effective_year\"],\n",
    "                    \"source\": os.path.basename(fpath),\n",
    "                    \"page\": (i+j)//5 + 1,\n",
    "                    \"section\": \"policy\",\n",
    "                    \"content\": ch,\n",
    "                    \"chunk_id\": f\"{os.path.basename(fpath)}-{i+j:04d}\"\n",
    "                }\n",
    "            })\n",
    "        index.upsert(vectors=points, namespace=\"policies\")\n",
    "        total += len(points)\n",
    "print(\"Total chunks upserted:\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2258e",
   "metadata": {},
   "source": [
    "## 5) Coverage Q&A â€” ask with plan/state/year filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Does my plan cover HVAC compressor?\"\n",
    "filters = {\"plan\": \"Gold\", \"state\": \"PA\", \"effective_year\": 2025}\n",
    "\n",
    "ctx, qv = retrieve(question, filters, k=8)\n",
    "print(\"Context hits:\", len(ctx))\n",
    "ctx_text = \"\\n\\n---\\n\\n\".join(f\"[{i}] {c['source']} p.{c['page']}\\n{c['content']}\" for i,c in enumerate(ctx))\n",
    "\n",
    "ANSWER_SYS = (\n",
    "    \"You are HomeShield AI. Answer ONLY using the provided context from policy documents. \"\n",
    "    'If unsupported, say you are unsure. Return JSON: '\n",
    "    '{\"answer\": string, \"citations\": [{\"source\": string, \"page\": number, \"quote\": string}]}.'\n",
    ")\n",
    "\n",
    "prompt = f\"Question: {question}\\n\\nContext:\\n{ctx_text}\"\n",
    "resp = client.chat.completions.create(\n",
    "    model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",\"gpt-4o\"),\n",
    "    messages=[{\"role\":\"system\",\"content\":ANSWER_SYS},{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339ff68",
   "metadata": {},
   "source": [
    "## 6) Claim submission â€” extract â†’ validate coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customers = pd.read_csv(CUSTOMERS_CSV)\n",
    "customer_id = customers.iloc[0][\"customer_id\"]\n",
    "plan  = customers.iloc[0][\"plan\"]\n",
    "state = customers.iloc[0][\"state\"]\n",
    "year  = int(str(customers.iloc[0][\"effective_date\"])[:4])\n",
    "\n",
    "message = \"My AC stopped cooling yesterday, thermostat shows E1.\"\n",
    "\n",
    "EXTRACT_SYS = (\n",
    "    \"Extract claim fields from text. Return STRICT JSON with keys: \"\n",
    "    '{\"appliance\": string, \"issue\": string, \"failure_date\": string | null}. If date not found, use null.'\n",
    ")\n",
    "extraction = client.chat.completions.create(\n",
    "    model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",\"gpt-4o\"),\n",
    "    messages=[{\"role\":\"system\",\"content\":EXTRACT_SYS},{\"role\":\"user\",\"content\":message}],\n",
    "    temperature=0\n",
    ").choices[0].message.content\n",
    "\n",
    "try:\n",
    "    extraction_json = json.loads(extraction)\n",
    "except:\n",
    "    import re as _re\n",
    "    cleaned = _re.sub(r\"^```json|```$\", \"\", extraction.strip(), flags=_re.MULTILINE)\n",
    "    extraction_json = json.loads(cleaned)\n",
    "\n",
    "q_for_ret = f\"{extraction_json.get('appliance','')} {extraction_json.get('issue','')}\".strip() or message\n",
    "ctx, _ = retrieve(q_for_ret, {\"plan\": plan, \"state\": state, \"effective_year\": year}, k=8)\n",
    "ctx_text = \"\\n\\n---\\n\\n\".join(f\"[{i}] {c['source']} p.{c['page']}\\n{c['content']}\" for i,c in enumerate(ctx))\n",
    "\n",
    "REASON_SYS = (\n",
    "    \"You are a coverage validator. Use ONLY the provided context. \"\n",
    "    'Return JSON: {\"decision\": \"covered\" | \"partial\" | \"denied\" | \"ambiguous\", '\n",
    "    '\"reasons\": [string], \"citations\": [{\"source\": string, \"page\": number, \"quote\": string}]}.'\n",
    ")\n",
    "reason_user = (\n",
    "    f\"Customer plan: {plan}, {state}, {year}\\n\"\n",
    "    f\"Appliance: {extraction_json.get('appliance')}\\n\"\n",
    "    f\"Issue: {extraction_json.get('issue')}\\n\"\n",
    "    f\"Failure date: {extraction_json.get('failure_date')}\\n\\n\"\n",
    "    f\"Context:\\n{ctx_text}\"\n",
    ")\n",
    "decision = client.chat.completions.create(\n",
    "    model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",\"gpt-4o\"),\n",
    "    messages=[{\"role\":\"system\",\"content\":REASON_SYS},{\"role\":\"user\",\"content\":reason_user}],\n",
    "    temperature=0\n",
    ").choices[0].message.content\n",
    "\n",
    "print(\"Extraction ->\", extraction_json)\n",
    "print(\"Decision ->\", decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbbf4d",
   "metadata": {},
   "source": [
    "## 7) (Optional) Quick evaluation on a few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4aa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gold_path = Path(r\"C:\\Users\\kalva\\AI_Projects\\HomeShield_AI\\homeshield_sample_data\\evaluation_pairs.jsonl\")\n",
    "if gold_path.exists():\n",
    "    rows = [json.loads(l) for l in gold_path.read_text(encoding=\"utf-8\").splitlines()][:5]\n",
    "    for r in rows:\n",
    "        q = r[\"question\"]\n",
    "        plan = r[\"metadata\"][\"plan\"]; state = r[\"metadata\"][\"state\"]; year = r[\"metadata\"][\"year\"]\n",
    "        ctx, _ = retrieve(q, {\"plan\":plan, \"state\":state, \"effective_year\":year}, k=8)\n",
    "        ctx_text = \"\\n\\n---\\n\\n\".join(f\"[{i}] {c['source']} p.{c['page']}\\n{c['content']}\" for i,c in enumerate(ctx))\n",
    "        prompt = f\"Question: {q}\\n\\nContext:\\n{ctx_text}\"\n",
    "        out = client.chat.completions.create(\n",
    "            model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\",\"gpt-4o\"),\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Answer only from context, return plain text.\"},\n",
    "                      {\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content\n",
    "        print(\"\\nQ:\", q, \"\\nModel:\", out, \"\\nExpected:\", r[\"expected_answer\"][:120], \"...\")\n",
    "else:\n",
    "    print(\"evaluation_pairs.jsonl not found â€” skipping.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}